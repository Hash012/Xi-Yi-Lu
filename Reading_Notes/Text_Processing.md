## 2.6
#### SAE探索目标语言现象的特征过程：
1. 根据目标语言现象构造语言数据集，$(s^+ ,s^−)$。
2. 经SAE的基向量$k$的激活为$a_k^{(1)}=a_k(s^+)$， $a_k^{(0)}=a_k(s^−)$, 每个基向量对每个句子的个体潜在效应为个体潜在效应为:$τ=a_k^{(1)}-a_k^{(0)}$，在该特定数据集上有$EALE_k=\sum_{i=1}^Nτ_k(s_i)$。通过计算每个基向量的$EALE_k$进行敏感性预过滤，并保留绝对值超过第75百分位数的那些。





3. 在SAE后的向量空间，定义二元触发器$Z_k(s) = \mathbb{I}[a_k(s) \geq \theta_k]$。

    **充分概率 (PS)**。
    对于基向量 $k$，添加现象使向量“开启”的概率为：
    $$\text{PS}_k = \Pr \left[ Z_k^{(1)} = 1 \mid Z_k^{(0)} = 0 \right],$$
    其中 $Z_k^{(1)}$ 和 $Z_k^{(0)}$ 分别在 $s^+$ 和 $s^-$ 上测量。

    **必要概率 (PN)**。
    相反，如果现象被移除，向量将关闭的概率为：
    $$\text{PN}_k = \Pr \left[ Z_k^{(0)} = 0 \mid Z_k^{(1)} = 1 \right].$$
    特征表征置信度 (FRC)。 我们用调和平均将两个因果概率结合，以惩罚仅充分或仅必要的向量：
    $$\text{FRC}_k = 2 \cdot \frac{\text{PS}_k \cdot \text{PN}_k}{\text{PS}_k + \text{PN}_k}.$$

    我们从每个 $\langle s^+, s^- \rangle$ 对估计 $\text{PS}_k$ 和 $\text{PN}_k$ 并按 $\text{FRC}_k$ 对向量排序；
4. 最后，将前10名向量的激活分布传递给LLM智能体，该智能体验证每个向量是否真正编码了预期的语言特征，并标记任何不一致或虚假的模式。

#### 通过SAE得到的的特征空间的理解
在Llama-3.1-8B 的 32 层中，
1. 每层特征空间的向量独立编号，不跨层互通
2. 每层特征空间的向量编号关于层是固定的。


## 2.12
#### ECHOES OF BERT
1. 背景：
先前关于基于Transformer的大型语言模型（LMs）何在内部表示语言学信息的研究仅关注第一代LMs（如BERT和GPT-2），这些工作表明其存在层次化组织，其中不同层专门捕获不同级别的语言学结构，从表面特征到句法和语义。

2. 所研究：
现代LMs如何编码语言学结构，这些表示与早期模型中的表示有何不同？特别是，现代LMs是否重新发现了先前探测工作所暗示的经典NLP流程？

重点研究：语言模型是按共享意义（walk、walked）还是按共享语法（walked、jumped）对单词进行分组？更广泛地说，LMs在哪里以及如何编码单词的词元（其词汇身份）及其语法形式（屈折形态）？

3. 研究方向：
- 在模型表示（隐藏状态激活）中对每种这些语言学属性进行了探测实验
- 在25个跨越不同架构、规模和训练机制的模型上训练分类器，以确定它们是否以层次化方式处理英语语言学结构
- 探测注意力头和预训练检查点，找到转向向量，并分析内在维度，以展示这两种语言学方面在层内的位置、它们在预训练期间如何演变

4. 发现：
现代LMs一致地发现了经典NLP流程：早期层处理表面和句法信息，中间层强调语义和实体级信息，后期层捕获话语。
词汇信息主要在早期层编码，并在网络更深层变得越来越非线性，而屈折形态在所有层和语言中保持线性可访问。
词汇身份和屈折形态表示在预训练早期出现，并且在架构、规模和语言上具有鲁棒性。特别是，我们发现屈折特征在残差流中占据紧凑的、线性可访问的子空间，从而实现有效的转向。